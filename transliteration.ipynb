{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["## IMPORTING LIBRARIES"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import requests,zipfile,io\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from torch import optim\n","import numpy as np\n","import random\n","import torch.nn.functional as F\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import wandb\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import csv"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## DOWNLOADING AND UNZIPPING DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def download_data(url=\"https://drive.google.com/u/0/uc?id=1uRKU4as2NlS9i8sdLRS1e326vQRdhvfw&export=download\"):\n","    response=requests.get(url)\n","    z = zipfile.ZipFile(io.BytesIO(response.content))\n","    z.extractall()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## METHODS FOR GETTING CHARACTERS FOR CORPUSS AND ADDING THEIR INDICES"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_corpus(data):\n","    eng_corpus=set()\n","    hin_corpus=set()\n","    for i in range(0,len(data)):\n","        eng_word=data[0][i]\n","        hin_word=data[1][i]\n","        for ch in eng_word:\n","            eng_corpus.add(ch)\n","        for ch in hin_word:\n","            hin_corpus.add(ch)\n","        # End Delimiter\n","        eng_corpus.add('#')\n","        hin_corpus.add('#')\n","        hin_corpus.add('$')\n","        eng_corpus.add('$')\n","        # Start Delimiter\n","#         eng_corpus.add('^')\n","        hin_corpus.add('^')\n","    return hin_corpus,eng_corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def word2index(data):\n","    hin_corpus,eng_corpus=get_corpus(data)\n","    engchar_idx={}\n","    hinchar_idx={}\n","    idx_engchar={}\n","    idx_hinchar={}\n","    i=0\n","    for char in eng_corpus:\n","        engchar_idx[char]=i\n","        idx_engchar[i]=char\n","        i+=1\n","    i=0\n","    for char in hin_corpus:\n","        hinchar_idx[char]=i\n","        idx_hinchar[i]=char\n","        i+=1\n","    return engchar_idx,hinchar_idx,idx_engchar,idx_hinchar,len(eng_corpus),len(hin_corpus)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## DATA PREPROCESSING"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def maxlen(data):\n","    maxlen_eng=0\n","    maxlen_hin=0\n","    for i in range(0,len(data)):\n","        eng_word=data[0][i]\n","        hin_word=data[1][i]\n","        if(len(eng_word)>maxlen_eng):\n","            maxlen_eng=len(eng_word)\n","        if(len(hin_word)>maxlen_hin):\n","            maxlen_hin=len(hin_word)\n","    return maxlen_eng,maxlen_hin"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def pre_process(data,eng_to_idx,hin_to_idx):\n","    eng=[]\n","    hin=[]\n","    maxlen_eng,maxlen_hin=maxlen(data)\n","    \n","    unknown= eng_to_idx['$']\n","    for i in range(0,len(data)):\n","        sz=0\n","        eng_word=data[0][i]\n","        hin_word='^'+data[1][i]\n","        eng_word = eng_word.ljust(maxlen_eng+1, '#')\n","        hin_word = hin_word.ljust(maxlen_hin+1, '#')\n","        idx=[]\n","        for char in eng_word:\n","            if eng_to_idx.get(char) is not None:\n","                idx.append(eng_to_idx[char])\n","            else:\n","                idx.append(unknown)\n","        eng.append(idx)\n","        idx=[]\n","        for char in hin_word:\n","            if hin_to_idx.get(char) is not None:\n","                idx.append(hin_to_idx[char])\n","            else:\n","                idx.append(unknown)\n","        hin.append(idx)    \n","    return eng,hin"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## LOADING OUR CUSTOM DATASET TO DATALOADER"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, train_x,train_y, transform=None):\n","        self.train_x = train_x\n","        self.train_y = train_y\n","        self.transform = transform\n","        \n","    \n","    def __len__(self):\n","        return len(self.train_x)\n","    \n","    def __getitem__(self, idx):\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return torch.tensor(self.train_x[idx]).to(device),torch.tensor(self.train_y[idx]).to(device)\n","\n","def get_data():\n","    download_data()\n","    \n","    train_df=pd.read_csv(\"aksharantar_sampled/hin/hin_train.csv\",header=None)\n","    test_df=pd.read_csv(\"aksharantar_sampled/hin/hin_test.csv\",header=None)\n","    val_df=pd.read_csv(\"aksharantar_sampled/hin/hin_valid.csv\",header=None)\n","    eng_to_idx,hin_to_idx,idx_to_eng,idx_to_hin,input_len,target_len=word2index(train_df)\n","    \n","    return train_df,test_df,val_df,eng_to_idx,hin_to_idx,idx_to_eng,idx_to_hin,input_len,target_len"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Seq2Seq MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class EncoderGRU(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_of_layers,batch_size,bi_directional,dropout_p=0.1):\n","        super(EncoderGRU,self).__init__()\n","        self.hidden_size=hidden_size\n","        self.batch_size=batch_size\n","        self.input_size=input_size\n","        self.embedding_size=embedding_size\n","        self.embedding=nn.Embedding(input_size,embedding_size)\n","        self.num_of_layers=num_of_layers\n","        self.bi_directional=bi_directional\n","        if(bi_directional==\"Yes\"):\n","            flag=True\n","        else:\n","            flag=False\n","        self.gru = nn.GRU(embedding_size,hidden_size,num_of_layers,bidirectional=flag)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self,input,hidden):\n","        embedded=self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n","        embedded = self.dropout(embedded)\n","        output,hidden=self.gru(embedded,hidden)\n","    \n","        if self.bi_directional==\"Yes\":\n","            hidden=hidden.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n","            hidden=torch.add(hidden[0],hidden[1])/2\n","            \n","        return output,hidden\n","\n","    def initHidden(self):\n","        if(self.bi_directional==\"Yes\"):\n","            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","        else:\n","            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","\n","class DecoderGRU(nn.Module):\n","    def __init__(self, output_size,hidden_size, embedding_size, decoder_layers,batch_size,dropout_p=0.1):\n","        super(DecoderGRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding_size=embedding_size\n","        self.embedding = nn.Embedding(output_size, embedding_size)\n","        self.gru = nn.GRU(embedding_size,hidden_size, decoder_layers,dropout = dropout_p)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.batch_size=batch_size\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n","#         embedded = self.dropout(embedded)\n","        output, hidden = self.gru(embedded, hidden)\n","        output = self.softmax(self.out(output))\n","        return output, hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_of_layers,batch_size,bi_directional,dropout_p=0.1):\n","        super(EncoderRNN,self).__init__()\n","        self.hidden_size=hidden_size\n","        self.batch_size=batch_size\n","        self.input_size=input_size\n","        self.embedding_size=embedding_size\n","        self.embedding=nn.Embedding(input_size,embedding_size)\n","        self.num_of_layers=num_of_layers\n","        self.bi_directional=bi_directional\n","        if(bi_directional==\"Yes\"):\n","            flag=True\n","        else:\n","            flag=False\n","        self.rnn = nn.RNN(embedding_size,hidden_size,num_of_layers,bidirectional=flag)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self,input,hidden):\n","        embedded=self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n","        embedded = self.dropout(embedded)\n","        output,hidden=self.rnn(embedded,hidden)\n","    \n","        if self.bi_directional==\"Yes\":\n","            hidden=hidden.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n","            hidden=torch.add(hidden[0],hidden[1])/2\n","            \n","        return output,hidden\n","\n","    def initHidden(self):\n","        if(self.bi_directional==\"Yes\"):\n","            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","        else:\n","            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, output_size,hidden_size, embedding_size, decoder_layers,batch_size,dropout_p=0.1):\n","        super(DecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding_size=embedding_size\n","        self.embedding = nn.Embedding(output_size, embedding_size)\n","        self.rnn = nn.RNN(embedding_size,hidden_size, decoder_layers,dropout = dropout_p)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.batch_size=batch_size\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n","#         embedded = self.dropout(embedded)\n","        output, hidden = self.rnn(embedded, hidden)\n","        output = self.softmax(self.out(output))\n","        return output, hidden"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class EncoderLSTM(nn.Module):\n","    def __init__(self,input_size,hidden_size,embedding_size,num_of_layers,batch_size,bi_directional,dropout_p=0.1):\n","        super(EncoderLSTM,self).__init__()\n","        self.hidden_size=hidden_size\n","        self.batch_size=batch_size\n","        self.input_size=input_size\n","        self.embedding_size=embedding_size\n","        self.embedding=nn.Embedding(input_size,embedding_size)\n","        self.num_of_layers=num_of_layers\n","        self.bi_directional=bi_directional\n","        if(bi_directional==\"Yes\"):\n","            flag=True\n","        else:\n","            flag=False\n","        self.lstm = nn.LSTM(embedding_size,hidden_size,num_of_layers,bidirectional=flag)\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self,input,hidden,state):\n","        embedded=self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n","        embedded = self.dropout(embedded)\n","        output,(hidden,state)=self.lstm(embedded,(hidden,state))\n","    \n","        if self.bi_directional==\"Yes\":\n","            hidden=hidden.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n","            state=state.resize(2,self.num_of_layers,self.batch_size,self.hidden_size)\n","            hidden=torch.add(hidden[0],hidden[1])/2\n","            state=torch.add(state[0],hidden[1])/2\n","            \n","        return output,hidden,state\n","\n","    def initHidden(self):\n","        if(self.bi_directional==\"Yes\"):\n","            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","        else:\n","            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","    \n","    def initState(self):\n","        if(self.bi_directional==\"Yes\"):\n","            return torch.zeros(2*self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","        else:\n","            return torch.zeros(self.num_of_layers,self.batch_size,self.hidden_size,device=device)\n","\n","class DecoderLSTM(nn.Module):\n","    def __init__(self, output_size,hidden_size, embedding_size, decoder_layers,batch_size,dropout_p=0.1):\n","        super(DecoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding_size=embedding_size\n","        self.embedding = nn.Embedding(output_size, embedding_size)\n","        self.lstm = nn.LSTM(embedding_size,hidden_size,decoder_layers,dropout = dropout_p)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        self.softmax = nn.LogSoftmax(dim=2)\n","        self.batch_size=batch_size\n","        self.dropout = nn.Dropout(dropout_p)\n","\n","    def forward(self, input,hidden,state):\n","        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n","#         embedded = self.dropout(embedded)\n","        output,(hidden,state)=self.lstm(embedded,(hidden,state))\n","        output = self.softmax(self.out(output))\n","        return output,hidden,state"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## ATTENTION MECHANISM"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class AttnDecoder(nn.Module):\n","    def __init__(self,output_size,hidden_size,embedding_size,decoder_layers,batch_size,cell_type,dropout_p=0.1):\n","        super(AttnDecoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.batch_size=batch_size\n","        self.cell_type=cell_type\n","        self.embedding_size=embedding_size\n","        self.decoder_layers=decoder_layers\n","        \n","        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","\n","        self.U=nn.Linear(self.hidden_size,self.hidden_size,bias=False).to(device)\n","        self.W=nn.Linear(self.hidden_size,self.hidden_size,bias=False).to(device)\n","        self.V=nn.Linear(self.hidden_size,1,bias=False).to(device)\n","        \n","        self.linear=nn.Linear(self.hidden_size,output_size,bias=True)\n","        self.softmax=nn.Softmax(dim=1)\n","        self.softmax1=nn.LogSoftmax(dim=2)\n","        \n","        if(cell_type==\"GRU\"):\n","            self.gru = nn.GRU(self.embedding_size+self.hidden_size, self.hidden_size,self.decoder_layers,dropout = dropout_p)\n","        if(cell_type==\"LSTM\"):\n","            self.lstm = nn.LSTM(self.embedding_size+self.hidden_size, self.hidden_size,self.decoder_layers,dropout = dropout_p)\n","        if(cell_type==\"RNN\"):\n","            self.rnn = nn.RNN(self.embedding_size+self.hidden_size, self.hidden_size,self.decoder_layers,dropout = dropout_p)\n","\n","    def forward(self, input, hidden,encoder_outputs,word_length,state=None):\n","        embedded = self.embedding(input).view(-1,self.batch_size, self.embedding_size)\n","        T=word_length\n","        temp1=self.W(hidden[-1])\n","        temp2=self.U(encoder_outputs)\n","        c=torch.zeros(self.batch_size,1,self.hidden_size).to(device)\n","        temp1=temp1.unsqueeze(0)\n","\n","        e_j=self.V(F.tanh(temp1+temp2))\n","        alpha_j=self.softmax(e_j)\n","        \n","        c = torch.bmm(alpha_j.permute(1,2,0),encoder_outputs.permute(1,0,2))\n","        \n","        final_input=torch.cat((embedded[0],c.squeeze(1)),1).unsqueeze(0)\n","    \n","        final_input = F.relu(final_input)\n","        \n","        if(self.cell_type==\"GRU\"):\n","            output,hidden=self.gru(final_input,hidden)\n","        if(self.cell_type==\"RNN\"):\n","            output,hidden=self.rnn(final_input,hidden)\n","        if(self.cell_type==\"LSTM\"):\n","            output, (hidden,state) =self.lstm(final_input,(hidden,state))\n","        \n","        \n","        output1=self.softmax1(self.linear(output))\n","        if(self.cell_type==\"GRU\" or self.cell_type==\"RNN\"):\n","            return output1, hidden, alpha_j\n","        if(self.cell_type==\"LSTM\"):\n","            return output1, hidden, state, alpha_j"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train(train_data,encoder,decoder,loss_fun,encoder_optimizer,decoder_optimizer,encoder_layers,decoder_layers,batch_size,hidden_size,bi_directional,cell_type,attention):\n","    total_loss=0\n","    teacher_forcing_ratio=0.5\n","    for i,(train_x,train_y) in enumerate(train_data):\n","        loss=0\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","        train_x=train_x.T\n","        train_y=train_y.T\n","        timesteps=len(train_x)\n","        \n","        if cell_type=='GRU' or cell_type=='RNN':\n","            \n","            encoder_hidden=encoder.initHidden()\n","            encoder_output,encoder_hidden=encoder(train_x,encoder_hidden)\n","            if(decoder_layers>encoder_layers):\n","                i = decoder_layers\n","                decoder_hidden=encoder_hidden\n","\n","                while True:\n","                    if(i==encoder_layers):\n","                        break\n","                    # Concatenate the two tensors along the first dimension\n","                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n","                    i-=1\n","\n","            elif(decoder_layers<encoder_layers):\n","                decoder_hidden=encoder_hidden[-decoder_layers:]\n","\n","            else:\n","                decoder_hidden=encoder_hidden\n","        \n","            decoder_input = train_y[0]\n","            \n","            if(bi_directional==\"Yes\"):\n","                split_tensor= torch.split(encoder_output, hidden_size, dim=-1)\n","                encoder_output=torch.add(split_tensor[0],split_tensor[1])/2\n","            \n","            \n","            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","            if use_teacher_forcing:\n","                for i in range(0,len(train_y)):\n","                    if(attention==\"Yes\"):\n","                        decoder_output, decoder_hidden, attn_weights=decoder(decoder_input,decoder_hidden,encoder_output,len(train_x))\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input = train_y[i] \n","                    else:\n","                        decoder_output, decoder_hidden= decoder(decoder_input, decoder_hidden)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input = train_y[i]  # Teacher forcing\n","            else:\n","                for i in range(0,len(train_y)):\n","                    if(attention==\"Yes\"):\n","                        decoder_output, decoder_hidden, attn_weights=decoder(decoder_input,decoder_hidden,encoder_output,len(train_x))\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input=index\n","                    else:\n","                        decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden)\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input=index\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            total_loss+=loss\n","        \n","        if cell_type=='LSTM':\n","    \n","            encoder_hidden=encoder.initHidden()\n","            encoder_state=encoder.initState()\n","            \n","            encoder_output,encoder_hidden,encoder_state=encoder(train_x,encoder_hidden,encoder_state)\n","        \n","            if(decoder_layers>encoder_layers):\n","                i = decoder_layers\n","                decoder_hidden=encoder_hidden\n","                decoder_state=encoder_state\n","                while True:\n","                    if(i==encoder_layers):\n","                        break\n","                    # Concatenate the two tensors along the first dimension\n","                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n","                    decoder_state = torch.cat([decoder_state, encoder_state[-1].unsqueeze(0)], dim=0)\n","                    i-=1\n","\n","            elif(decoder_layers<encoder_layers):\n","                decoder_hidden=encoder_hidden[-decoder_layers:]\n","                decoder_state=encoder_state[-decoder_layers:]\n","\n","            else:\n","                decoder_hidden=encoder_hidden\n","                decoder_state=encoder_state\n","            \n","            \n","            if(bi_directional==\"Yes\"):\n","                split_tensor= torch.split(encoder_output, hidden_size, dim=-1)\n","                encoder_output=torch.add(split_tensor[0],split_tensor[1])/2\n","            \n","            decoder_input = train_y[0]\n","            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","            if use_teacher_forcing:\n","                for i in range(0,len(train_y)):\n","                    if(attention==\"Yes\"):\n","                        decoder_output, decoder_hidden, decoder_state, attn_weights=decoder(decoder_input,decoder_hidden,encoder_output,len(train_x),decoder_state)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input= train_y[i]\n","                    else:\n","                        decoder_output, decoder_hidden,decoder_state= decoder(decoder_input, decoder_hidden,decoder_state)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input = train_y[i]  # Teacher forcing\n","            else:\n","                for i in range(0,len(train_y)):\n","                    if(attention==\"Yes\"):\n","                        decoder_output, decoder_hidden, decoder_state, attn_weights=decoder(decoder_input,decoder_hidden,encoder_output,len(train_x),decoder_state)\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input=index\n","                    else:\n","                        decoder_output, decoder_hidden,decoder_state= decoder(decoder_input, decoder_hidden,decoder_state)\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), train_y[i])\n","                        decoder_input=index\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            total_loss+=loss\n","\n","        \n","        \n","    return total_loss.item()/len(train_y),encoder,decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train_iter(input_data,val_data,val_y,input_len,target_len,epochs,batch_size,embedding_size,encoder_layers,decoder_layers,hidden_size,cell_type,bi_directional,dropout,attention,beam_size=0):\n","    lr=0.001\n","    if(cell_type=='GRU'):\n","        encoder=EncoderGRU(input_len,hidden_size,embedding_size,encoder_layers,batch_size,bi_directional,dropout).to(device)\n","        if(attention==\"Yes\"):\n","            decoder=AttnDecoder(target_len,hidden_size,embedding_size,decoder_layers,batch_size,cell_type,dropout).to(device)\n","        else:\n","            decoder=DecoderGRU(target_len,hidden_size,embedding_size,decoder_layers,batch_size,dropout).to(device)\n","        \n","    if(cell_type=='RNN'):\n","        encoder=EncoderRNN(input_len,hidden_size,embedding_size,encoder_layers,batch_size,bi_directional,dropout).to(device)\n","        if(attention==\"Yes\"):\n","            decoder=AttnDecoder(target_len,hidden_size,embedding_size,decoder_layers,batch_size,cell_type,dropout).to(device)\n","        else:\n","            decoder=DecoderRNN(target_len,hidden_size,embedding_size,decoder_layers,batch_size,dropout).to(device)\n","    \n","    if cell_type=='LSTM':\n","        encoder=EncoderLSTM(input_len,hidden_size,embedding_size,encoder_layers,batch_size,bi_directional,dropout).to(device)\n","        if(attention==\"Yes\"):\n","            decoder=AttnDecoder(target_len,hidden_size,embedding_size,decoder_layers,batch_size,cell_type,dropout).to(device)\n","        else:\n","            decoder=DecoderLSTM(target_len,hidden_size,embedding_size,decoder_layers,batch_size,dropout).to(device)\n","\n","    encoder_optimizer=optim.Adam(encoder.parameters(),lr)\n","    decoder_optimizer=optim.Adam(decoder.parameters(),lr)\n","    loss_fun=nn.CrossEntropyLoss(reduction=\"sum\")\n","    epoch_train_loss=[]\n","    epoch_val_loss=[]\n","    epoch_val_acc=[]\n","    for i in range(0,epochs):\n","        loss,encoder,decoder=train(input_data,encoder,decoder,loss_fun,encoder_optimizer,decoder_optimizer,\n","                                   encoder_layers,decoder_layers,batch_size,hidden_size,bi_directional,\n","                                   cell_type,attention)\n","        val_predictions,val_loss,attn_weights=eval(val_data,encoder,decoder,encoder_layers,decoder_layers,\n","                                  batch_size,hidden_size,bi_directional,cell_type,attention)\n","        \n","        epoch_val_loss.append(val_loss)\n","        epoch_train_loss.append(loss/51200)\n","        \n","        val_acc=accuracy(val_predictions,val_y)\n","        epoch_val_acc.append(val_acc)\n","        print(loss/51200,val_loss,val_acc)\n","    \n","#     train_predictions,t=eval(input_data,encoder,decoder,encoder_layers,decoder_layers,batch_size,hidden_size,bi_directional,cell_type,attention)\n","    return epoch_train_loss,epoch_val_loss,epoch_val_acc,encoder,decoder,encoder_layers,decoder_layers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def eval(input_data,encoder,decoder,encoder_layers,decoder_layers,batch_size,hidden_size,bi_directional,cell_type,attention,build_matrix=False):\n","    with torch.no_grad():\n","        loss_fun=nn.CrossEntropyLoss(reduction=\"sum\")\n","        total_loss=0\n","        pred_words=list()\n","        attention_matrix=[]\n","        for x,y in input_data:\n","            attn=[]\n","            loss=0\n","            decoder_words=[]\n","            x=x.T\n","            y=y.T\n","            encoder_hidden=encoder.initHidden()\n","            timesteps=len(x)\n","            if cell_type=='GRU' or cell_type=='RNN':\n","\n","                encoder_hidden=encoder.initHidden()\n","                encoder_output,encoder_hidden=encoder(x,encoder_hidden)\n","                if(decoder_layers>encoder_layers):\n","                    i = decoder_layers\n","                    decoder_hidden=encoder_hidden\n","\n","                    while True:\n","                        if(i==encoder_layers):\n","                            break\n","                        # Concatenate the two tensors along the first dimension\n","                        decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n","                        i-=1\n","\n","                elif(decoder_layers<encoder_layers):\n","                    decoder_hidden=encoder_hidden[-decoder_layers:]\n","\n","                else:\n","                    decoder_hidden=encoder_hidden\n","\n","                decoder_input = y[0]\n","\n","                if(bi_directional==\"Yes\"):\n","                    split_tensor= torch.split(encoder_output, hidden_size, dim=-1)\n","                    encoder_output=torch.add(split_tensor[0],split_tensor[1])/2\n","\n","                for i in range(0,len(y)):\n","                    if(attention==\"Yes\"):\n","                        decoder_output, decoder_hidden, attn_weights=decoder(decoder_input,decoder_hidden,encoder_output,len(x))\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), y[i])\n","                        index=index.squeeze()\n","                        decoder_input=index\n","                        decoder_words.append(index.tolist())\n","                        if(build_matrix==True):\n","                            attn.append(attn_weights)\n","                    else:\n","                        decoder_output,decoder_hidden=decoder(decoder_input,decoder_hidden)\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), y[i])\n","                        index=index.squeeze()\n","                        decoder_input=index\n","                        decoder_words.append(index.tolist())\n","                if(build_matrix==True):\n","                    attention_matrix=torch.cat(tuple(x for x in attn),dim=2).to(device)\n","                decoder_words=np.array(decoder_words)\n","                pred_words.append(decoder_words.T)\n","                total_loss+=loss.item()\n","\n","\n","            if cell_type=='LSTM':\n","\n","                encoder_hidden=encoder.initHidden()\n","                encoder_state=encoder.initState()\n","\n","                encoder_output,encoder_hidden,encoder_state=encoder(x,encoder_hidden,encoder_state)\n","\n","                if(decoder_layers>encoder_layers):\n","                    i = decoder_layers\n","                    decoder_hidden=encoder_hidden\n","                    decoder_state=encoder_state\n","                    while True:\n","                        if(i==encoder_layers):\n","                            break\n","                        # Concatenate the two tensors along the first dimension\n","                        decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n","                        decoder_state = torch.cat([decoder_state, encoder_state[-1].unsqueeze(0)], dim=0)\n","                        i-=1\n","\n","                elif(decoder_layers<encoder_layers):\n","                    decoder_hidden=encoder_hidden[-decoder_layers:]\n","                    decoder_state=encoder_state[-decoder_layers:]\n","\n","                else:\n","                    decoder_hidden=encoder_hidden\n","                    decoder_state=encoder_state\n","\n","\n","                if(bi_directional==\"Yes\"):\n","                    split_tensor= torch.split(encoder_output, hidden_size, dim=-1)\n","                    encoder_output=torch.add(split_tensor[0],split_tensor[1])/2\n","                decoder_input = y[0]\n","\n","                for i in range(0,len(y)):\n","                    if(attention==\"Yes\"):\n","                        decoder_output, decoder_hidden, decoder_state, attn_weights=decoder(decoder_input,decoder_hidden,encoder_output,len(x),decoder_state)\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), y[i])\n","                        index=index.squeeze()\n","                        decoder_input=index\n","                        decoder_words.append(index.tolist())\n","                        if(build_matrix==True):\n","                            attn.append(attn_weights)\n","                    else:\n","                        decoder_output, decoder_hidden,decoder_state= decoder(decoder_input, decoder_hidden,decoder_state)\n","                        max_prob,index=decoder_output.topk(1)\n","                        loss+=loss_fun(torch.squeeze(decoder_output), y[i])\n","                        index=index.squeeze()\n","                        decoder_input=index\n","                        decoder_words.append(index.tolist())\n","                if(build_matrix==True):\n","                    attention_matrix=torch.cat(tuple(x for x in attn),dim=2).to(device)\n","                decoder_words=np.array(decoder_words)\n","                pred_words.append(decoder_words.T)\n","                total_loss+=loss.item()\n","\n","\n","    predictions=[]\n","    for batch in pred_words:\n","        for word in batch:\n","            predictions.append(word)\n","    \n","    return predictions,total_loss/(len(predictions)*len(predictions[0])),attention_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def accuracy(predictions,y):\n","    count=0\n","    for i in range(0,len(predictions)):\n","        p=predictions[i]\n","        if np.array_equal(p,y[i]):\n","            count+=1\n","    return (count/len(predictions))*100"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## INTEGRATING WITH WANDB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def wandb_run_sweeps(train_dataset,val_dataset,test_dataset,train_y,val_y,test_y,input_len,target_len):\n","    \n","    config = {\n","        \"project\":\"CS6910_Assignment3\",\n","        \"method\": 'bayes',\n","        \"metric\": {\n","        'name': 'acc',\n","        'goal': 'maximize'\n","        },\n","        'parameters' :{\n","        \"epochs\": {\"values\":[15,20,25]},\n","        \"batchsize\": {\"values\": [64,128,256]},\n","        \"embedding_size\": {\"values\":[256, 512,1024]},\n","        \"hidden_size\": {\"values\":[256, 512,1024]},\n","        \"encoder_layers\": {\"values\":[2,3,4]},\n","        \"decoder_layers\": {\"values\":[2,3,4]},\n","        \"cell_type\": {\"values\":[\"LSTM\"]},\n","        \"bi_directional\":{\"values\":[\"Yes\"]},\n","        \"dropout\":{\"values\":[0.2,0.3,0.5]},\n","        \"attention\":{\"values\":[\"No\"]},\n","        }\n","    }\n","    def train_rnn():\n","        wandb.init()\n","\n","        name='_CT_'+str(wandb.config.cell_type)+\"_BS_\"+str(wandb.config.batchsize)+\"_EPOCH_\"+str(wandb.config.epochs)+\"_ES_\"+str(wandb.config.embedding_size)+\"_HS_\"+str(wandb.config.hidden_size)\n","        \n","        \n","        train_dataloader=DataLoader(train_dataset,batch_size=wandb.config.batchsize)\n","        test_dataloader=DataLoader(test_dataset,batch_size=wandb.config.batchsize)\n","        val_dataloader=DataLoader(val_dataset,batch_size=wandb.config.batchsize)\n","        \n","        epoch_train_loss,epoch_val_loss,epoch_val_acc,encoder,decoder,encoder_layers,decoder_layers=train_iter(train_dataloader,val_dataloader,val_y,input_len,target_len,wandb.config.epochs,wandb.config.batchsize,wandb.config.embedding_size,wandb.config.encoder_layers,wandb.config.decoder_layers,wandb.config.hidden_size,wandb.config.cell_type,wandb.config.bi_directional,wandb.config.dropout,wandb.config.attention)\n","\n","        for i in range(wandb.config.epochs):\n","            wandb.log({\"loss\":epoch_train_loss[i]})\n","            wandb.log({\"val_loss\":epoch_val_loss[i]})\n","            wandb.log({\"val_acc\":epoch_val_acc[i]})\n","            wandb.log({\"epoch\": (i+1)})\n","        wandb.log({\"validation_accuracy\":epoch_val_acc[-1]})    \n","        \n","        train_predictions,_,_=eval(train_dataloader,encoder,decoder,wandb.config.encoder_layers,\n","                              wandb.config.decoder_layers,wandb.config.batchsize,wandb.config.hidden_size,\n","                              wandb.config.bi_directional,wandb.config.cell_type,wandb.config.attention)\n","\n","        train_accuracy=accuracy(train_predictions,train_y)\n","        wandb.log({\"train_accuracy\":train_accuracy})\n","        \n","        test_predictions,_,_=eval(test_dataloader,encoder,decoder,wandb.config.encoder_layers,\n","                              wandb.config.decoder_layers,wandb.config.batchsize,wandb.config.hidden_size,\n","                              wandb.config.bi_directional,wandb.config.cell_type,wandb.config.attention)\n","\n","        test_accuracy=accuracy(test_predictions,test_y)\n","        wandb.log({\"test_accuracy\":test_accuracy})\n","        wandb.log({\"acc\":epoch_val_acc[-1]})\n","        wandb.run.name = name\n","        wandb.run.save()\n","        wandb.run.finish()\n","    wandb.login(key=\"aecb4b665a37b40204530b0627a42274aeddd3e1\")\n","    sweep_id=wandb.sweep(config,project=\"CS6910_Assignment3\")\n","    wandb.agent(sweep_id,function=train_rnn)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def wandb_run_configuration(train_dataset,val_dataset,test_dataset,train_y,val_y,test_x,test_y,epochs,encoder_layers,decoder_layers,batchsize,embedding_size,hidden_size,bi_directional,dropout,cell_type,attention):\n","    \n","    wandb.login(key = \"aecb4b665a37b40204530b0627a42274aeddd3e1\")\n","    wandb.init(project=\"CS6910_Assignment3\")\n","    name='_CT_'+str(cell_type)+\"_BS_\"+str(batchsize)+\"_EPOCH_\"+str(epochs)+\"_ES_\"+str(embedding_size)+\"_HS_\"+str(hidden_size)\n","\n","\n","    train_dataloader=DataLoader(train_dataset,batch_size=batchsize)\n","    test_dataloader=DataLoader(test_dataset,batch_size=batchsize)\n","    val_dataloader=DataLoader(val_dataset,batch_size=batchsize)\n","\n","    epoch_train_loss,epoch_val_loss,epoch_val_acc,encoder,decoder,encoder_layers,decoder_layers=train_iter(train_dataloader,val_dataloader,val_y,input_len,target_len,epochs,batchsize,embedding_size,encoder_layers,decoder_layers,hidden_size,cell_type,bi_directional,dropout,attention)\n","\n","    for i in range(epochs):\n","        wandb.log({\"loss\":epoch_train_loss[i]})\n","        wandb.log({\"val_loss\":epoch_val_loss[i]})\n","        wandb.log({\"val_acc\":epoch_val_acc[i]})\n","        wandb.log({\"epoch\": (i+1)})\n","    wandb.log({\"validation_accuracy\":epoch_val_acc[-1]})    \n","\n","    train_predictions,_,_=eval(train_dataloader,encoder,decoder,encoder_layers,decoder_layers,batchsize,hidden_size,bi_directional,cell_type,attention)\n","\n","    train_accuracy=accuracy(train_predictions,train_y)\n","    wandb.log({\"train_accuracy\":train_accuracy})\n","\n","    test_predictions,_,_=eval(test_dataloader,encoder,decoder,encoder_layers,decoder_layers,batchsize,hidden_size,bi_directional,cell_type,attention)\n","    test_accuracy=accuracy(test_predictions,test_y)\n","    wandb.log({\"test_accuracy\":test_accuracy})\n","    wandb.log({\"acc\":epoch_val_acc[-1]})\n","    \n","    \n","    test_dataset_attn=MyDataset(test_x[:batchsize],test_y[:batchsize])\n","    test_dataloader_attn_for_matrix=DataLoader(test_dataset_attn,batch_size=batchsize)\n","    test_predictions,_,attn_matrix=eval(test_dataloader_attn_for_matrix,encoder,decoder,encoder_layers,decoder_layers,batchsize,hidden_size,bi_directional,cell_type,attention,True)\n","\n","    \n","    fig=plot_attention(test_predictions,attn_matrix)\n","    fig.savefig(\"ex.png\")\n","    temp = plt.imread(\"ex.png\")\n","    plt.show()\n","    image = wandb.Image(temp)\n","    wandb.log({\"attention heatmaps\":image})\n","    wandb.run.name = name\n","    wandb.run.save()\n","    wandb.run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def main():\n","    train_df,test_df,val_df,eng_to_idx,hin_to_idx,idx_to_eng,idx_to_hin,input_len,target_len=get_data()\n","\n","    train_x,train_y = pre_process(train_df,eng_to_idx,hin_to_idx)\n","    test_x,test_y = pre_process(test_df,eng_to_idx,hin_to_idx)\n","    val_x,val_y = pre_process(val_df,eng_to_idx,hin_to_idx)\n","\n","    train_dataset=MyDataset(train_x,train_y)\n","    test_dataset=MyDataset(test_x,test_y)\n","    val_dataset=MyDataset(val_x,val_y)\n","\n","    wandb_run_sweeps(train_dataset,val_dataset,test_dataset,train_y,val_y,test_y,input_len,target_len)\n","\n","\n","if __name__==\"__main__\":\n","    main()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def representation_to_hin_word(predictions,idx_to_hin):\n","    words=[]\n","    for word in predictions:\n","        s=''\n","        for char in word:\n","            if(idx_to_hin[char]!='#' and idx_to_hin[char]!='^'):\n","                s+=idx_to_hin[char]\n","        words.append(s)\n","    return words\n","\n","def representation_to_eng_word(predictions,idx_to_eng):\n","    words=[]\n","    for word in predictions:\n","        s=''\n","        for char in word:\n","            if(idx_to_eng[char]!='#' and idx_to_eng[char]!='^'):\n","                s+=idx_to_eng[char]\n","        words.append(s)\n","    return words"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_csv_file(test_x,test_y,test_predictions,idx_to_eng,idx_to_hin):\n","    test_eng_words=representation_to_eng_word(test_x,idx_to_eng)\n","    pred_test_words=representation_to_hin_word(test_predictions,idx_to_hin)\n","    test_hin_words=representation_to_hin_word(test_y,idx_to_hin)\n","    # Sample data to write to the CSV file\n","    data1 = [\n","        ['Input', 'Expected', 'Predicted']\n","    ]\n","\n","\n","    for i in range(len(test_eng_words)):\n","        data1.append([test_eng_words[i],pred_test_words[i],test_hin_words[i]])\n","    # Specify the file path and name for the CSV file\n","    csv_file_path = 'data1.csv'\n","\n","    # Open the CSV file in write mode\n","    with open(csv_file_path, mode='w', newline='') as file:\n","        # Create a CSV writer object\n","        writer = csv.writer(file)\n","\n","        # Write the data to the CSV file row by row\n","        for row in data1:\n","            writer.writerow(row)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_attention(test_predictions,attn_matrix):\n","    \n","    attn_matrix1=attn_matrix.permute(1,0,2)\n","    attn_matrix1=attn_matrix1[:9]\n","    total_words,input_length,output_length = attn_matrix1.shape\n","\n","\n","    from matplotlib.font_manager import FontProperties\n","\n","\n","    tel_font = FontProperties(fname = '/kaggle/input/hindi-font/TiroDevanagariHindi-Regular.ttf')\n","\n","\n","    fig, axes = plt.subplots(3, 3, figsize=(12,12))\n","\n","    fig.tight_layout(pad=5.0)\n","    fig.subplots_adjust(top=0.90)\n","    axes = axes.ravel()\n","\n","    for i in range(total_words):\n","        count=0\n","        start1=0\n","        end1=0\n","        eng_word=\"\"\n","        for char in test_x[i]:\n","            if(idx_to_eng[char]=='^'):\n","                start1=count+1\n","            elif(idx_to_eng[char]=='#'):\n","                end1=count\n","                break\n","            else:\n","                eng_word+=idx_to_eng[char]\n","            count+=1\n","\n","        count=0\n","        hin_word=\"\"\n","        for char in test_predictions[i]:\n","            if(idx_to_hin[char]=='^'):\n","                start=count+1\n","            elif(idx_to_hin[char]=='#'):\n","                end=count\n","                break\n","            else:\n","                hin_word+=idx_to_hin[char]\n","            count+=1\n","\n","        attn=attn_matrix1[i,start1:end1,start:end].cpu().numpy()\n","        sns.heatmap(attn, ax=axes[i],cmap=\"Greens\")\n","        axes[i].set_yticklabels(eng_word,rotation=10)  \n","        axes[i].set_xticklabels(hin_word,fontproperties = tel_font,fontdict={'fontsize':16})\n","        axes[i].xaxis.tick_top()\n","    \n","    return fig"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# epochs=30\n","# batchsize=256\n","# embedding_size=256\n","# encoder_layers=3\n","# decoder_layers=3\n","# hidden_size=1024\n","# cell_type=\"LSTM\"\n","# bi_directional=\"No\"\n","# dropout=0.1\n","# attention=\"Yes\""]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
